(kmeans)
kmeans는 시작할 때 랜덤으로 센트로이드를 초기화하여
시도할 때마다 결과가 달라지고 최적의 군집을 도출해내지 못하는 문제가 있다.

하드 군집은 transform 사용하여 가장 가까운 클러스터를 선택
소프트 군집은 np.linalg.norm을 사용하여 클러스터마다 샘플에 점수를 부여


이너셔도 transform을 사용하여 구한다.
이너셔는 kmeans의 성능을 평가할 방법
score 메서드가 음의 값이 이유는 항상 " 큰 값이 좋은 것 " 규칙을 따라야 하기 때문이다.

미니배치는 kmeans보다 더 빠르지만 이너셔값은 더 낮게 나올 수 있다.

최적의 클러스터 개수를 설정하기 위해 클러스터 개수에 대한 이너셔 그래프를 이용해 관성으로 개수를 정할 수 있지만
클러스터 개수가 증가할 수록 관성이 작아져 실루엣 점수를 사용함

실루엣 계수가 1에 가까우면 자신의 클러스터 안에 포함되고
0은 클러스터 경계, -1은 잘못된 클러스터에 할당된 것이다.
실루엣 다이어그램을 이용해 최적의 클러스터 개수를 찾을 수 있다.

kmeans는 전처리 단계에서 사용하여 차원 축소의 효과도 볼 수 있다.



(DBSCAN)
DBSCAN의 파라미터
eps : ԑ-이웃 범위 ( 주어진 기준값 ԑ 반경 내에 위치한 샘플)
min_samples : ԑ 반경 내에 위치하는 이웃의 수




(GaussianMixture)
kmeans는 타원형 군집에 약하기 때문에 GaussianMixture을 사용한다.

가우시안 혼합모델
각 샘플이 어떤 정규분포를 따르는지 파악

gm = GaussianMixture(n_components=3, n_init=10, random_state=42)
gm.fit(X)
샘플만 알고 레이블, 가중치 파라미터, 평균, 공분산 행렬을 모르는 상태에서
무작위 추정 후 n_init 만큼 수렴할 때까지 반복시킨다.

gm.converged_ 로 수렴했는지 확인

em 알고리즘
1. kmeans 처럼 라벨링이 안 되어 있는 상태에서 랜덤한 분포를 제안
2. likehood 비교로 라벨링 (사전 지식이 있는 상태에서 데이터가 들어왔을 때 확률)
3. 각 그룹별 모수 추정
4. 추정된 모수를 이용한 각 그룹별 분포 도시
2번으로 다시 반복 수행


최적의 클러스터 개수 선택

군집이 타원형일 때 값이 일정하지 않기 때문에 관성이나 실루엣 점수를 사용하지 못 한다.
BIC와 AIC를 이용하여 그래프에서 최소가 되는 클러스터 개수를 선택한다.

 BayesianGaussianMixture도 타원형에서 사용가능하다.
